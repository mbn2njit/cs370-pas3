{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
      },
      "source": [
        "# UAV Detection and Tracking\n",
        "\n",
        "Multi-Object Tracking (MOT) is a core visual ability that humans poses\n",
        "to perform kinetic tasks and coordinate other tasks. The AI community\n",
        "has recognized the importance of MOT via a series of\n",
        "[competitions](https://motchallenge.net).\n",
        "\n",
        "In this assignment, the object class is `drone` and the ability to track\n",
        "this object will be demonstrated using [Kalman\n",
        "Filters](https://en.wikipedia.org/wiki/Kalman_filter). The assignment\n",
        "will give you the opportunity to apply probabilistic reasoning in the\n",
        "physical security application space.\n",
        "\n",
        "## Task 1: Setup your development environment and store the test videos locally (10 points)"
      ],
      "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!nvidia-smi\n",
        "from ultralytics import YOLO\n",
        "import ultralytics\n",
        "import os\n",
        "import cv2\n",
        "ultralytics.checks()\n",
        "\n",
        "#model = YOLO('yolov8n.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VoxD7MsA4ax",
        "outputId": "3767eea9-57b0-4264-c81d-27c54d8e06e9"
      },
      "id": "9VoxD7MsA4ax",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.212 üöÄ Python-3.10.12 torch-2.1.0+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 26.9/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube\n",
        "from pytube import YouTube\n",
        "import os\n",
        "\n",
        "VIDEO_FOLDER = \"droneVids\"\n",
        "vidLink1 = YouTube(\"https://www.youtube.com/watch?v=DhmZ6W1UAv4\")\n",
        "vidLink2 = YouTube(\"https://www.youtube.com/watch?v=YrydHPwRelI\")\n",
        "\n",
        "if not os.path.exists(VIDEO_FOLDER):\n",
        "    os.makedirs(VIDEO_FOLDER)\n",
        "\n",
        "stream1 = vidLink1.streams.get_by_itag(22)\n",
        "stream2 = vidLink2.streams.get_by_itag(22)\n",
        "\n",
        "stream1.download(\"droneVids\")\n",
        "stream2.download(\"droneVids\")\n",
        "\n",
        "import cv2\n",
        "\n",
        "#this variable downloads a frame every downloadOnWhats frame\n",
        "#think animating on 15s, or animating on 4s\n",
        "#this is to prevent neural networking literally tens of\n",
        "#thousands of images\n",
        "\"\"\"downloadOnWhats = 15\n",
        "\n",
        "for filename in os.listdir(\"droneVids\"):\n",
        "    f = os.path.join(\"droneVids\", filename)\n",
        "    if os.path.isfile(f):\n",
        "        print(f)\n",
        "    capture = cv2.VideoCapture(f)\n",
        "    fDir, nothin = os.path.splitext(f)\n",
        "    if not os.path.exists(fDir):\n",
        "            os.makedirs(fDir)\n",
        "    frameNr = 0\n",
        "    count = 1\n",
        "    formNr = \"{:05d}\".format(count)\n",
        "    while (True):\n",
        "        success, frame = capture.read()\n",
        "        if(frameNr % downloadOnWhats == 0):\n",
        "            if success:\n",
        "                cv2.imwrite(f'{fDir}/frame_{formNr}.jpg', frame)\n",
        "                count += 1\n",
        "            else:\n",
        "                break\n",
        "        frameNr = frameNr+1\n",
        "        formNr = \"{:05d}\".format(count)\n",
        "\n",
        "    capture.release()\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "kEnO8vX27o_T",
        "outputId": "51b4d0e0-639b-465f-8322-4f2a0edd4b2d"
      },
      "id": "kEnO8vX27o_T",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'downloadOnWhats = 15\\n\\nfor filename in os.listdir(\"droneVids\"):\\n    f = os.path.join(\"droneVids\", filename)\\n    if os.path.isfile(f):\\n        print(f)\\n    capture = cv2.VideoCapture(f)\\n    fDir, nothin = os.path.splitext(f)\\n    if not os.path.exists(fDir):\\n            os.makedirs(fDir)\\n    frameNr = 0\\n    count = 1\\n    formNr = \"{:05d}\".format(count)\\n    while (True):\\n        success, frame = capture.read()\\n        if(frameNr % downloadOnWhats == 0):\\n            if success:\\n                cv2.imwrite(f\\'{fDir}/frame_{formNr}.jpg\\', frame)\\n                count += 1\\n            else:\\n                break\\n        frameNr = frameNr+1\\n        formNr = \"{:05d}\".format(count)\\n\\n    capture.release()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/gdrive/MyDrive/DroneDataset.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"/content/DroneDataset\")\n"
      ],
      "metadata": {
        "id": "-CG524jwH_l0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef78ccd0-84dd-457b-97ce-694889094853"
      },
      "id": "-CG524jwH_l0",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Drone Object Detection (40 points)\n",
        "\n",
        "You need to research can use any dataset that can be used to detect the\n",
        "class `drone` such as the drones used for the test videos. Please be\n",
        "careful to distinguish between the datasets that detect objects *from*\n",
        "drones to datasets that detect *the* drones. Your object detector must\n",
        "use a deep learning model but you can use an existing object detector\n",
        "model architecture.\n",
        "\n",
        "Split the videos into frames and use each frame to present the drone\n",
        "detections you got. Store all images that you had detections in a folder\n",
        "called `detections`. Write your code in such a way that a number of\n",
        "videos can be processed from a directory and not just these two."
      ],
      "metadata": {
        "id": "LTXBTtS3Avw6"
      },
      "id": "LTXBTtS3Avw6"
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8s.pt data=/content/DroneDataset/drone_dataset/data.yaml epochs=3 plots=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5lYiYYqE0SV",
        "outputId": "33b838b2-f48e-42d8-c89a-b6df67d2e7ee"
      },
      "id": "c5lYiYYqE0SV",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.212 üöÄ Python-3.10.12 torch-2.1.0+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/content/DroneDataset/drone_dataset/data.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 40.4MB/s]\n",
            "2023-11-19 19:33:27.687723: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-19 19:33:27.687807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-19 19:33:27.687855: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "Model summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/DroneDataset/drone_dataset/train/labels... 1012 images, 0 backgrounds, 0 corrupt: 100% 1012/1012 [00:01<00:00, 880.34it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/DroneDataset/drone_dataset/train/images/pic_722.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/DroneDataset/drone_dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/DroneDataset/drone_dataset/valid/labels... 347 images, 0 backgrounds, 0 corrupt: 100% 347/347 [00:00<00:00, 934.83it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/DroneDataset/drone_dataset/valid/labels.cache\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3         0G       1.82      4.287      1.939         36        640:  11% 7/64 [04:59<39:30, 41.58s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        1/3         0G      1.557      2.943      1.736         34        640:  28% 18/64 [12:16<30:26, 39.71s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        1/3         0G      1.447      2.117      1.623         42        640:  70% 45/64 [29:51<11:54, 37.61s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        1/3         0G      1.448      1.914      1.623          8        640: 100% 64/64 [41:35<00:00, 38.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 11/11 [03:15<00:00, 17.81s/it]\n",
            "                   all        347        369      0.678      0.504      0.537      0.207\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3         0G      1.496      1.393      1.645         41        640:  56% 36/64 [23:08<17:45, 38.06s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        2/3         0G      1.511      1.405       1.66         47        640:  70% 45/64 [28:47<11:52, 37.50s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        2/3         0G      1.516      1.397      1.665         48        640:  81% 52/64 [33:07<07:25, 37.15s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        2/3         0G      1.492      1.372      1.655         10        640: 100% 64/64 [40:04<00:00, 37.56s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 11/11 [03:11<00:00, 17.45s/it]\n",
            "                   all        347        369      0.546      0.518      0.491      0.229\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3         0G      1.416      1.237      1.576         46        640:  50% 32/64 [20:10<19:24, 36.40s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        3/3         0G      1.412      1.208      1.561         42        640:  75% 48/64 [30:01<10:04, 37.80s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        3/3         0G      1.409       1.21      1.564         39        640:  83% 53/64 [33:10<06:56, 37.85s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
            "        3/3         0G      1.402      1.203       1.56         14        640: 100% 64/64 [39:26<00:00, 36.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 11/11 [03:07<00:00, 17.01s/it]\n",
            "                   all        347        369      0.793      0.702      0.745      0.294\n",
            "\n",
            "3 epochs completed in 2.181 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.212 üöÄ Python-3.10.12 torch-2.1.0+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 11/11 [03:03<00:00, 16.66s/it]\n",
            "                   all        347        369      0.794      0.702      0.745      0.294\n",
            "Speed: 1.6ms preprocess, 497.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('/content/runs/detect/train2/weights/best.pt')\n",
        "\n",
        "resultGenerator = model(source, save=True, stream=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "KiWywF0X7ljf",
        "outputId": "c71dbed1-bc75-4809-e811-4b70aa10174a"
      },
      "id": "KiWywF0X7ljf",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dd2598a65afd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresultGenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "if not os.path.exists(\"detections\"):\n",
        "    os.makedirs(\"detections\")\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model = YOLO('/content/runs/detect/train2/weights/best.pt')\n",
        "\n",
        "for filename in os.listdir('/content/droneVids'):\n",
        "    video_path = os.path.join('/content/droneVids', filename)\n",
        "\n",
        "    if os.path.isfile(video_path):\n",
        "        dirName, nothin = os.path.splitext(filename)\n",
        "        downloadDir = os.path.join('/content/detections', dirName)\n",
        "        print(video_path)\n",
        "        if not os.path.exists(downloadDir):\n",
        "            os.makedirs(downloadDir)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "\n",
        "        frameNr = 0\n",
        "        # Loop through the video frames\n",
        "        while cap.isOpened():\n",
        "            # Read a frame from the video\n",
        "            success, frame = cap.read()\n",
        "            frameNr += 1\n",
        "            if success:\n",
        "                # Run YOLOv8 inference on the frame\n",
        "                results = (model(frame))[0]\n",
        "\n",
        "                for classes in results.boxes:\n",
        "                    if(classes.cls == 0 and classes.conf >= 0.3):\n",
        "                        # Visualize the results on the frame\n",
        "                        annotated_frame = results.plot()\n",
        "\n",
        "                        # Writes frame to detections\n",
        "                        formNr = \"{:05d}\".format(frameNr)\n",
        "                        cv2.imwrite(f'{downloadDir}/frame_{formNr}.jpg', annotated_frame)\n",
        "\n",
        "                        # Display the annotated frame\n",
        "                        #cv2_imshow(annotated_frame)\n",
        "                        break\n",
        "\n",
        "\n",
        "                # Break the loop if 'q' is pressed\n",
        "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "                    break\n",
        "            else:\n",
        "                # Break the loop if the end of the video is reached\n",
        "                break\n",
        "\n",
        "        # Release the video capture object and close the display window\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "XCCW6D6luJd3"
      },
      "id": "XCCW6D6luJd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/detections.zip /content/detections\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/detections.zip\")"
      ],
      "metadata": {
        "id": "0t9ZPDZiIgAb"
      },
      "id": "0t9ZPDZiIgAb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This script is for deleting entire folders worth of frames\n",
        "#Do not run this script if going through this ipynb as intended\n",
        "\n",
        "\"\"\"fDir = '/content/detections/Drone tracking 2'\n",
        "for filename in os.listdir(fDir):\n",
        "    f = os.path.join(fDir, filename)\n",
        "    if os.path.isfile(f):\n",
        "        os.remove(f)\"\"\""
      ],
      "metadata": {
        "id": "O5YUYVI7PBGJ"
      },
      "id": "O5YUYVI7PBGJ",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Kalman Filter (50 points)\n",
        "\n",
        "Use the \\`filterpy\\`\\` library to implement a Kalman filter that will\n",
        "track the drone in the video. You will need to use the detections from\n",
        "the previous task to initialize the Kalman filter.\n",
        "\n",
        "You need to deliver a number of short videos with each video containing\n",
        "**only** the frames where the drone is present in the test video and its\n",
        "2D trajectory shown as a line that connects the pixels that the tracker\n",
        "indicated. You can use the `ffmpeg` command line tool and OpenCV to\n",
        "superpose the bounding box of the drone on the video as well as plot its\n",
        "trajectory."
      ],
      "metadata": {
        "id": "epbUdh8oA7wd"
      },
      "id": "epbUdh8oA7wd"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from filterpy.kalman import KalmanFilter\n",
        "from filterpy.common import Q_discrete_white_noise"
      ],
      "metadata": {
        "id": "CEGQFFV1Ri72"
      },
      "id": "CEGQFFV1Ri72",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"kalmanDetections\"):\n",
        "    os.makedirs(\"kalmanDetections\")\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model = YOLO('/content/runs/detect/train2/weights/best.pt')\n",
        "\n",
        "for filename in os.listdir('/content/droneVids'):\n",
        "    video_path = os.path.join('/content/droneVids', filename)\n",
        "\n",
        "    if os.path.isfile(video_path):\n",
        "        dirName, nothin = os.path.splitext(filename)\n",
        "        downloadDir = os.path.join('/content/kalmanDetections', dirName)\n",
        "        print(video_path)\n",
        "        if not os.path.exists(downloadDir):\n",
        "            os.makedirs(downloadDir)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        frameNr = 0\n",
        "        # Loop through the video frames\n",
        "        while cap.isOpened():\n",
        "            # Read a frame from the video\n",
        "            success, frame = cap.read()\n",
        "            frameNr += 1\n",
        "            if success:\n",
        "                # Run YOLOv8 inference on the frame\n",
        "                results = (model(frame))[0]\n",
        "\n",
        "                for classes in results.boxes:\n",
        "                    if(classes.cls == 0 and classes.conf >= 0.3):\n",
        "                        # Visualize the results on the frame\n",
        "                        annotated_frame = results.plot()\n",
        "\n",
        "                        #grab my bounding box\n",
        "                        x1, y1, x2, y2 = classes.xyxy[0]\n",
        "                        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "                        # Writes frame to detections\n",
        "                        formNr = \"{:05d}\".format(frameNr)\n",
        "                        cv2.imwrite(f'{downloadDir}/frame_{formNr}.jpg', annotated_frame)\n",
        "\n",
        "                        # Display the annotated frame\n",
        "                        #cv2_imshow(annotated_frame)\n",
        "                        break\n",
        "\n",
        "\n",
        "                # Break the loop if 'q' is pressed\n",
        "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "                    break\n",
        "            else:\n",
        "                # Break the loop if the end of the video is reached\n",
        "                break\n",
        "\n",
        "        # Release the video capture object and close the display window\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "BKMQF8AiA8cv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "966e7124-8eb5-406f-bd95-884ff78f0b75"
      },
      "id": "BKMQF8AiA8cv",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/droneVids/Drone tracking 2.mp4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e38ab2d4fbc7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_age\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_hits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mframeNr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Loop through the video frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sort' is not defined"
          ]
        }
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}